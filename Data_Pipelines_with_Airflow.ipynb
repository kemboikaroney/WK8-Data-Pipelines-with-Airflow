{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCy8I19jatVm7WYGEvjn9M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kemboikaroney/WK8-Data-Pipelines-with-Airflow/blob/main/Data_Pipelines_with_Airflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Data Pipelines with Airflow for MTN Rwanda\n",
        "'''\n",
        "from airflow import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import psycopg2\n",
        "import logging\n",
        "\n",
        "# Define the default arguments for the DAG\n",
        "default_args = {\n",
        "    'owner': 'MTN Rwanda Telecoms',\n",
        "    'depends_on_past': False,\n",
        "    'start_date': datetime(2023, 3, 19),\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "    'retry_delay': timedelta(minutes=5)\n",
        "}\n",
        "\n",
        "# Define the DAG\n",
        "dag = DAG('data_pipeline', \n",
        "          default_args=default_args, \n",
        "          schedule_interval=timedelta(days=1)\n",
        "          )\n",
        "\n",
        "# Define the function to extract the data from the CSV files\n",
        "def extract_data():\n",
        "    # Load the customer, order, and payment data from CSV files into Pandas dataframes\n",
        "    customer_df = pd.read_csv('customer_data.csv')\n",
        "    order_df = pd.read_csv('order_data.csv')\n",
        "    payment_df = pd.read_csv('payment_data.csv')\n",
        "    # Return the dataframes\n",
        "    return customer_df, order_df, payment_df\n",
        "\n",
        "# Define the function to transform the data\n",
        "def transform_data(customer_df, order_df, payment_df):\n",
        "    # Convert the date_of_birth field to datetime format\n",
        "    customer_df['date_of_birth'] = pd.to_datetime(customer_df['date_of_birth'])\n",
        "\n",
        "    # Merge the customer and order dataframes on the customer_id column\n",
        "    customer_order_df = pd.merge(customer_df, order_df, on='customer_id')\n",
        "\n",
        "    # Merge the payment dataframe with the merged dataframe on the order_id and customer_id columns\n",
        "    customer_payment_df = pd.merge(customer_order_df, payment_df, on=['order_id', 'customer_id'])\n",
        "\n",
        "    # Drop unnecessary columns like customer_id and order_id\n",
        "    customer_payment_df.drop(columns=['customer_id', 'order_id'], inplace=True)\n",
        "\n",
        "    # Group the data by customer and aggregate the amount paid using sum\n",
        "    customer_grouped_df = customer_payment_df.groupby(['first_name', 'last_name', 'email', 'country', 'gender', 'date_of_birth'])['amount'].sum().reset_index()\n",
        "\n",
        "    # Create a new column to calculate the total value of orders made by each customer\n",
        "    customer_grouped_df['total_order_value'] = customer_payment_df.groupby(['first_name', 'last_name', 'email', 'country', 'gender', 'date_of_birth'])['price'].sum().values\n",
        "    # Calculate the customer lifetime value using the formula CLV = (average order value) x (number of orders made per year) x (average customer lifespan)\n",
        "    customer_grouped_df['average_order_value'] = customer_grouped_df['total_order_value'] / customer_grouped_df['amount']\n",
        "    customer_grouped_df['number_of_orders_per_year'] = customer_grouped_df['amount'] / ((pd.to_datetime('now') - customer_grouped_df['date_of_birth']).dt.days / 365)\n",
        "    customer_grouped_df['average_customer_lifespan'] = (pd.to_datetime('now') - customer_grouped_df['date_of_birth']).dt.days / 365\n",
        "    customer_grouped_df['clv'] = customer_grouped_df['average_order_value'] * customer_grouped_df['number_of_orders_per_year'] * customer_grouped_df['average_customer_lifespan']\n",
        "    # Return the transformed dataframe\n",
        "    return customer_grouped_df\n",
        "\n",
        "\n",
        "# Define the function to load the transformed data into a PostgreSQL database\n",
        "# Use try catch block to catch any error in loading the data and log the error using python logging module\n",
        "\n",
        "def load_data(transformed_df):\n",
        "    '''\n",
        "    Logs a success message when the data is loaded successfully into the PostgreSQL database, and an error message with the specific error when an error occurs. \n",
        "    '''\n",
        "    try:\n",
        "        # Connect to the PostgreSQL database\n",
        "        conn = psycopg2.connect(\n",
        "            host=\"your_host\",\n",
        "            database=\"your_database\",\n",
        "            user=\"your_username\",\n",
        "            password=\"your_password\"\n",
        "        )\n",
        "\n",
        "        # Open a cursor to perform database operations\n",
        "        cur = conn.cursor()\n",
        "\n",
        "        # Create the customer_ltv table\n",
        "        cur.execute(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS customer_ltv (\n",
        "                customer_id INTEGER PRIMARY KEY,\n",
        "                total_orders INTEGER,\n",
        "                total_amount NUMERIC(10,2),\n",
        "                avg_order_value NUMERIC(10,2),\n",
        "                ltv NUMERIC(10,2)\n",
        "            )\n",
        "        \"\"\")\n",
        "\n",
        "        # Insert the transformed data into the customer_ltv table\n",
        "        for index, row in transformed_df.iterrows():\n",
        "            cur.execute(\"\"\"\n",
        "                INSERT INTO customer_ltv (customer_id, total_orders, total_amount, avg_order_value, ltv)\n",
        "                VALUES (%s, %s, %s, %s, %s)\n",
        "            \"\"\", (row['customer_id'], row['total_orders'], row['total_amount'], row['avg_order_value'], row['ltv']))\n",
        "\n",
        "        # Commit the transaction\n",
        "        conn.commit()\n",
        "\n",
        "        # Close the cursor and connection to the database\n",
        "        cur.close()\n",
        "        conn.close()\n",
        "\n",
        "        # Log a success message\n",
        "        logging.info(\"Data loaded successfully into PostgreSQL database\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # Log an error message\n",
        "        logging.error(f\"Error loading data into PostgreSQL database: {str(e)}\")\n",
        "        raise e\n",
        "\n",
        "# Define the extract data task\n",
        "extract_data_task = PythonOperator(\n",
        "    task_id='extract_data',\n",
        "    python_callable=extract_data,\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "# Define the transform data task\n",
        "transform_data_task = PythonOperator(\n",
        "    task_id='transform_data',\n",
        "    python_callable=transform_data,\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "# Define the load data task\n",
        "load_data_task = PythonOperator(\n",
        "    task_id='load_data',\n",
        "    python_callable=load_data,\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "# Define the task dependencies\n",
        "extract_data_task >> transform_data_task >> load_data_task\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "PGaZdtwBKiCQ",
        "outputId": "1dc75480-aa7b-42fb-bf71-74583885276d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;33m<\u001b[0m\u001b[1;33mipython-input-\u001b[0m\u001b[1;33m14\u001b[0m\u001b[1;33m-02f4bd465048\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m20\u001b[0m\u001b[1;33m RemovedInAirflow3Warning\u001b[0m\u001b[33m: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">&lt;ipython-input-</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">14</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">-02f4bd465048&gt;:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">20</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> RemovedInAirflow3Warning</span><span style=\"color: #808000; text-decoration-color: #808000\">: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Task(PythonOperator): load_data>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}